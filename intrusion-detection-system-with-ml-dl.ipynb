{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":394223,"sourceType":"datasetVersion","datasetId":174616}],"dockerImageVersionId":30235,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement\n\n<img src=https://www.vmware.com/content/dam/digitalmarketing/vmware/en/images/gallery/thumbnails/tn-vmware-overview-of-nsx-distributed-ids.jpg width=600/>\n\nWith the dramatic growth of computer networks usage and the huge increase in the number of applications running on top of it, network security is becoming increasingly while the all the systems suffers from security vulnerabilities, which could increase the attacks that could negatively affects the economy.\nTherefore detecting vulnerabilities in the system in the network has been more important and need to be done as accurate as possible in real time.\nin this notebook a model will be created and trained using SVM classifier to distengush if there is an attack or not in the network packet.\n\n\n## Intrusion detection systems\n\nAn Intrusion Detection System (IDS) is a system that monitors network traffic for suspicious activity and issues alerts when such activity is discovered. It is a software application that scans a network or a system for the harmful activity or policy breaching. Any malicious venture or violation is normally reported either to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system integrates outputs from multiple sources and uses alarm filtering techniques to differentiate malicious activity from false alarms.\n\n<img src=https://www.temok.com/blog/wp-content/uploads/2020/10/network2.jpg width=600/>\n\n* Host-Based IDS (HIDS): A host-based IDS is deployed on a particular endpoint and designed to protect it against internal and external threats. Such an IDS may have the ability to monitor network traffic to and from the machine, observe running processes, and inspect the system’s logs. A host-based IDS’s visibility is limited to its host machine, decreasing the available context for decision-making, but has deep visibility into the host computer’s internals.\n* Network-Based IDS (NIDS): A network-based IDS solution is designed to monitor an entire protected network. It has visibility into all traffic flowing through the network and makes determinations based upon packet metadata and contents. This wider viewpoint provides more context and the ability to detect widespread threats; however, these systems lack visibility into the internals of the endpoints that they protect.\n\nDetection Method of IDS:\n\n* Signature-based Method:\nSignature-based IDS detects the attacks on the basis of the specific patterns such as number of bytes or number of 1’s or number of 0’s in the network traffic. It also detects on the basis of the already known malicious instruction sequence that is used by the malware. The detected patterns in the IDS are known as signatures.\nSignature-based IDS can easily detect the attacks whose pattern (signature) already exists in system but it is quite difficult to detect the new malware attacks as their pattern (signature) is not known.\n\n* Anomaly-based Method:\nAnomaly-based IDS was introduced to detect unknown malware attacks as new malware are developed rapidly. In anomaly-based IDS there is use of machine learning to create a trustful activity model and anything coming is compared with that model and it is declared suspicious if it is not found in model. Machine learning-based method has a better-generalized property in comparison to signature-based IDS as these models can be trained according to the applications and hardware configurations.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras import regularizers\nimport xgboost as xgb\nfrom sklearn.decomposition import PCA\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn import metrics\npd.set_option('display.max_columns',None)\nwarnings.filterwarnings('ignore')\n%matplotlib inline ","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:15.415441Z","iopub.execute_input":"2022-09-16T14:44:15.416175Z","iopub.status.idle":"2022-09-16T14:44:15.429261Z","shell.execute_reply.started":"2022-09-16T14:44:15.416114Z","shell.execute_reply":"2022-09-16T14:44:15.427707Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploring the dataset","metadata":{}},{"cell_type":"code","source":"# Read Train and Test dataset\ndata_train = pd.read_csv(\"../input/nslkdd/KDDTrain+.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:15.430957Z","iopub.execute_input":"2022-09-16T14:44:15.431372Z","iopub.status.idle":"2022-09-16T14:44:16.002515Z","shell.execute_reply.started":"2022-09-16T14:44:15.431334Z","shell.execute_reply":"2022-09-16T14:44:16.000581Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check data\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:16.004169Z","iopub.execute_input":"2022-09-16T14:44:16.004578Z","iopub.status.idle":"2022-09-16T14:44:16.044498Z","shell.execute_reply.started":"2022-09-16T14:44:16.004534Z","shell.execute_reply":"2022-09-16T14:44:16.043028Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = (['duration','protocol_type','service','flag','src_bytes','dst_bytes','land','wrong_fragment','urgent','hot'\n,'num_failed_logins','logged_in','num_compromised','root_shell','su_attempted','num_root','num_file_creations'\n,'num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login','count','srv_count','serror_rate'\n,'srv_serror_rate','rerror_rate','srv_rerror_rate','same_srv_rate','diff_srv_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count'\n,'dst_host_same_srv_rate','dst_host_diff_srv_rate','dst_host_same_src_port_rate','dst_host_srv_diff_host_rate','dst_host_serror_rate'\n,'dst_host_srv_serror_rate','dst_host_rerror_rate','dst_host_srv_rerror_rate','outcome','level'])","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:18.651854Z","iopub.execute_input":"2022-09-16T14:44:18.652356Z","iopub.status.idle":"2022-09-16T14:44:18.660055Z","shell.execute_reply.started":"2022-09-16T14:44:18.652313Z","shell.execute_reply":"2022-09-16T14:44:18.65877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assign name for columns\ndata_train.columns = columns","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:19.052385Z","iopub.execute_input":"2022-09-16T14:44:19.052972Z","iopub.status.idle":"2022-09-16T14:44:19.06098Z","shell.execute_reply.started":"2022-09-16T14:44:19.052915Z","shell.execute_reply":"2022-09-16T14:44:19.059543Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:19.49297Z","iopub.execute_input":"2022-09-16T14:44:19.493389Z","iopub.status.idle":"2022-09-16T14:44:19.533193Z","shell.execute_reply.started":"2022-09-16T14:44:19.493354Z","shell.execute_reply":"2022-09-16T14:44:19.531919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:19.552863Z","iopub.execute_input":"2022-09-16T14:44:19.553316Z","iopub.status.idle":"2022-09-16T14:44:19.617622Z","shell.execute_reply.started":"2022-09-16T14:44:19.553276Z","shell.execute_reply":"2022-09-16T14:44:19.616391Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train.describe().style.background_gradient(cmap='Blues').set_properties(**{'font-family':'Segoe UI'})","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:19.800875Z","iopub.execute_input":"2022-09-16T14:44:19.801352Z","iopub.status.idle":"2022-09-16T14:44:20.224028Z","shell.execute_reply.started":"2022-09-16T14:44:19.801311Z","shell.execute_reply":"2022-09-16T14:44:20.222695Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train.loc[data_train['outcome'] == \"normal\", \"outcome\"] = 'normal'\ndata_train.loc[data_train['outcome'] != 'normal', \"outcome\"] = 'attack'","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:20.22656Z","iopub.execute_input":"2022-09-16T14:44:20.227396Z","iopub.status.idle":"2022-09-16T14:44:20.267124Z","shell.execute_reply.started":"2022-09-16T14:44:20.227344Z","shell.execute_reply":"2022-09-16T14:44:20.265644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pie_plot(df, cols_list, rows, cols):\n    fig, axes = plt.subplots(rows, cols)\n    for ax, col in zip(axes.ravel(), cols_list):\n        df[col].value_counts().plot(ax=ax, kind='pie', figsize=(15, 15), fontsize=10, autopct='%1.0f%%')\n        ax.set_title(str(col), fontsize = 12)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:20.462704Z","iopub.execute_input":"2022-09-16T14:44:20.463143Z","iopub.status.idle":"2022-09-16T14:44:20.470427Z","shell.execute_reply.started":"2022-09-16T14:44:20.463104Z","shell.execute_reply":"2022-09-16T14:44:20.469166Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pie_plot(data_train, ['protocol_type', 'outcome'], 1, 2)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:20.902232Z","iopub.execute_input":"2022-09-16T14:44:20.902917Z","iopub.status.idle":"2022-09-16T14:44:21.182301Z","shell.execute_reply.started":"2022-09-16T14:44:20.902877Z","shell.execute_reply":"2022-09-16T14:44:21.181445Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing the data","metadata":{}},{"cell_type":"code","source":"def Scaling(df_num, cols):\n    std_scaler = RobustScaler()\n    std_scaler_temp = std_scaler.fit_transform(df_num)\n    std_df = pd.DataFrame(std_scaler_temp, columns =cols)\n    return std_df","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:21.294874Z","iopub.execute_input":"2022-09-16T14:44:21.295296Z","iopub.status.idle":"2022-09-16T14:44:21.302217Z","shell.execute_reply.started":"2022-09-16T14:44:21.295262Z","shell.execute_reply":"2022-09-16T14:44:21.300651Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_cols = ['is_host_login','protocol_type','service','flag','land', 'logged_in','is_guest_login', 'level', 'outcome']\ndef preprocess(dataframe):\n    df_num = dataframe.drop(cat_cols, axis=1)\n    num_cols = df_num.columns\n    scaled_df = Scaling(df_num, num_cols)\n    \n    dataframe.drop(labels=num_cols, axis=\"columns\", inplace=True)\n    dataframe[num_cols] = scaled_df[num_cols]\n    \n    dataframe.loc[dataframe['outcome'] == \"normal\", \"outcome\"] = 0\n    dataframe.loc[dataframe['outcome'] != 0, \"outcome\"] = 1\n    \n    dataframe = pd.get_dummies(dataframe, columns = ['protocol_type', 'service', 'flag'])\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:21.478371Z","iopub.execute_input":"2022-09-16T14:44:21.478957Z","iopub.status.idle":"2022-09-16T14:44:21.48808Z","shell.execute_reply.started":"2022-09-16T14:44:21.47888Z","shell.execute_reply":"2022-09-16T14:44:21.486396Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaled_train = preprocess(data_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:21.680606Z","iopub.execute_input":"2022-09-16T14:44:21.681718Z","iopub.status.idle":"2022-09-16T14:44:22.081701Z","shell.execute_reply.started":"2022-09-16T14:44:21.681664Z","shell.execute_reply":"2022-09-16T14:44:22.0804Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Principal Component Analysis\n\nPrincipal component analysis, or PCA, is a statistical technique to convert high dimensional data to low dimensional data by selecting the most important features that capture maximum information about the dataset. The features are selected on the basis of variance that they cause in the output. The feature that causes highest variance is the first principal component. The feature that is responsible for second highest variance is considered the second principal component, and so on. It is important to mention that principal components do not have any correlation with each other.\n\n<img src = https://hands-on.cloud/wp-content/uploads/2022/02/Implementing-Principal-component-analysis-PCA-using-Python.png width = \"600\"/>\n\n#### Advantages of PCA\n\nThere are two main advantages of dimensionality reduction with PCA.\n\n* The training time of the algorithms reduces significantly with less number of features.\n* It is not always possible to analyze data in high dimensions. For instance if there are 100 features in a dataset. Total number of scatter plots required to visualize the data would be 100(100-1)2 = 4950. Practically it is not possible to analyze data this way.\n","metadata":{}},{"cell_type":"code","source":"x = scaled_train.drop(['outcome', 'level'] , axis = 1).values\ny = scaled_train['outcome'].values\ny_reg = scaled_train['level'].values\n\npca = PCA(n_components=20)\npca = pca.fit(x)\nx_reduced = pca.transform(x)\nprint(\"Number of original features is {} and of reduced features is {}\".format(x.shape[1], x_reduced.shape[1]))\n\ny = y.astype('int')\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\nx_train_reduced, x_test_reduced, y_train_reduced, y_test_reduced = train_test_split(x_reduced, y, test_size=0.2, random_state=42)\nx_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(x, y_reg, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:22.085645Z","iopub.execute_input":"2022-09-16T14:44:22.086891Z","iopub.status.idle":"2022-09-16T14:44:24.836341Z","shell.execute_reply.started":"2022-09-16T14:44:22.086847Z","shell.execute_reply":"2022-09-16T14:44:24.835085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kernal_evals = dict()\ndef evaluate_classification(model, name, X_train, X_test, y_train, y_test):\n    train_accuracy = metrics.accuracy_score(y_train, model.predict(X_train))\n    test_accuracy = metrics.accuracy_score(y_test, model.predict(X_test))\n    \n    train_precision = metrics.precision_score(y_train, model.predict(X_train))\n    test_precision = metrics.precision_score(y_test, model.predict(X_test))\n    \n    train_recall = metrics.recall_score(y_train, model.predict(X_train))\n    test_recall = metrics.recall_score(y_test, model.predict(X_test))\n    \n    kernal_evals[str(name)] = [train_accuracy, test_accuracy, train_precision, test_precision, train_recall, test_recall]\n    print(\"Training Accuracy \" + str(name) + \" {}  Test Accuracy \".format(train_accuracy*100) + str(name) + \" {}\".format(test_accuracy*100))\n    print(\"Training Precesion \" + str(name) + \" {}  Test Precesion \".format(train_precision*100) + str(name) + \" {}\".format(test_precision*100))\n    print(\"Training Recall \" + str(name) + \" {}  Test Recall \".format(train_recall*100) + str(name) + \" {}\".format(test_recall*100))\n    \n    actual = y_test\n    predicted = model.predict(X_test)\n    confusion_matrix = metrics.confusion_matrix(actual, predicted)\n    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['normal', 'attack'])\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    ax.grid(False)\n    cm_display.plot(ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:24.838338Z","iopub.execute_input":"2022-09-16T14:44:24.838719Z","iopub.status.idle":"2022-09-16T14:44:24.851103Z","shell.execute_reply.started":"2022-09-16T14:44:24.838685Z","shell.execute_reply":"2022-09-16T14:44:24.849727Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modeling\nThe process of modeling means training a machine learning algorithm to predict the labels from the features, tuning it for the business need, and validating it on holdout data. The output from modeling is a trained model that can be used for inference, making predictions on new data points.\n\n<img src=\"https://docs.microsoft.com/en-us/windows/ai/images/winml-model-flow.png\" width=\"600\"/>\n\nA machine learning model itself is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data.\nOnce you have trained the model, you can use it to reason over data that it hasn't seen before, and make predictions about those data. For example, let's say you want to build an application that can recognize a user's emotions based on their facial expressions. You can train a model by providing it with images of faces that are each tagged with a certain emotion, and then you can use that model in an application that can recognize any user's emotion","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression\nThis type of statistical model (also known as logit model) is often used for classification and predictive analytics. Logistic regression estimates the probability of an event occurring, such as voted or didn’t vote, based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1. In logistic regression, a logit transformation is applied on the odds—that is, the probability of success divided by the probability of failure. This is also commonly known as the log odds, or the natural logarithm of odds, and this logistic function is represented by the following formulas:\n\n<img src = https://miro.medium.com/max/888/1*D4CIwT2zRCcVq88iji6qYQ.png />\n\nIn this logistic regression equation, h is the dependent or response variable and x is the independent variable. The beta parameter, or coefficient, in this model is commonly estimated via maximum likelihood estimation (MLE). This method tests different values of beta through multiple iterations to optimize for the best fit of log odds. All of these iterations produce the log likelihood function, and logistic regression seeks to maximize this function to find the best parameter estimate. Once the optimal coefficient (or coefficients if there is more than one independent variable) is found, the conditional probabilities for each observation can be calculated, logged, and summed together to yield a predicted probability. For binary classification, a probability less than .5 will predict 0 while a probability greater than 0 will predict 1.  After the model has been computed, it’s best practice to evaluate the how well the model predicts the dependent variable, which is called goodness of fit. \n\n#### Binary logistic regression:\n\nIn this approach, the response or dependent variable is dichotomous in nature—i.e. it has only two possible outcomes (e.g. 0 or 1). Some popular examples of its use include predicting if an e-mail is spam or not spam or if a tumor is malignant or not malignant. Within logistic regression, this is the most commonly used approach, and more generally, it is one of the most common classifiers for binary classification.\n\n#### Multinomial logistic regression:\n\nIn this type of logistic regression model, the dependent variable has three or more possible outcomes; however, these values have no specified order.  For example, movie studios want to predict what genre of film a moviegoer is likely to see to market films more effectively. A multinomial logistic regression model can help the studio to determine the strength of influence a person's age, gender, and dating status may have on the type of film that they prefer. The studio can then orient an advertising campaign of a specific movie toward a group of people likely to go see it.","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression().fit(x_train, y_train)\nevaluate_classification(lr, \"Logistic Regression\", x_train, x_test, y_train, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  k-nearest neighbors\nThe k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.\n\n<img src = https://miro.medium.com/max/1400/1*56MmutHmr4WzDJrufP8kug.png width = 600/>\n\n### Determine your distance metrics\n\nIn order to determine which data points are closest to a given query point, the distance between the query point and the other data points will need to be calculated. These distance metrics help to form decision boundaries, which partitions query points into different regions. You commonly will see decision boundaries visualized with Voronoi diagram.\n\n<img src = https://i.stack.imgur.com/JKtab.png/>","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=20).fit(x_train, y_train)\nevaluate_classification(knn, \"KNeighborsClassifier\", x_train, x_test, y_train, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Naive Bayes\nNaive Bayes classifiers are a collection of classification algorithms based on Bayes’ Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle. Every pair of features being classified is independent of each other.\nThe assumptions made by Naive Bayes are not generally correct in real-world situations. In-fact, the independence assumption is never correct but often works well in practice.\n\nNow, it is important to know about Bayes’ theorem.\n\n### Bayes’ Theorem\n\nBayes’ Theorem finds the probability of an event occurring given the probability of another event that has already occurred. Bayes’ theorem is stated mathematically as the following equation:\n\n<img src = https://hands-on.cloud/wp-content/uploads/2022/01/Implementing-Naive-Bayes-Classification-using-Python.png width=600/>\n\nwhere A and B are events and P(B) ≠ 0.\n\n* Basically, we are trying to find probability of event A, given the event B is true. Event B is also termed as evidence.\n* P(A) is the priori of A (the prior probability, i.e. Probability of event before evidence is seen). The evidence is an attribute value of an unknown instance(here, it is event B).\n* P(A|B) is a posteriori probability of B, i.e. probability of event after evidence is seen.","metadata":{}},{"cell_type":"code","source":"gnb = GaussianNB().fit(x_train, y_train)\nevaluate_classification(gnb, \"GaussianNB\", x_train, x_test, y_train, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Support Vector Machines\n\nSupport Vector Machine (SVM) is a relatively simple Supervised Machine Learning Algorithm used for classification and/or regression. It is more preferred for classification but is sometimes very useful for regression as well. Basically, SVM finds a hyper-plane that creates a boundary between the types of data. In 2-dimensional space, this hyper-plane is nothing but a line. In SVM, we plot each data item in the dataset in an N-dimensional space, where N is the number of features/attributes in the data. Next, find the optimal hyperplane to separate the data. So by this, you must have understood that inherently, SVM can only perform binary classification (i.e., choose between two classes). However, there are various techniques to use for multi-class problems. Support Vector Machine for Multi-CLass Problems To perform SVM on multi-class problems, we can create a binary classifier for each class of the data. The two results of each classifier will be :\n\n* The data point belongs to that class OR\n* The data point does not belong to that class.\n\n<img src=https://lh5.googleusercontent.com/Fqswkk7bY0_7EhTgbPWNOexWWzzDplNKYc1bJl6MI_edNVdwXnWI5xkoSyl7SxFHLLAWlW1AIEhWF2ilVvThzuU0HoPBNF79HsPezpXnKTv3DCFnD9ZeVPLmR828_JXpv5cB35TL width=600/>\n\nFor example, in a class of fruits, to perform multi-class classification, we can create a binary classifier for each fruit. For say, the ‘mango’ class, there will be a binary classifier to predict if it IS a mango OR it is NOT a mango. The classifier with the highest score is chosen as the output of the SVM. SVM for complex (Non Linearly Separable) SVM works very well without any modifications for linearly separable data. Linearly Separable Data is any data that can be plotted in a graph and can be separated into classes using a straight line.\n\nWe use Kernelized SVM for non-linearly separable data. Say, we have some non-linearly separable data in one dimension. We can transform this data into two dimensions and the data will become linearly separable in two dimensions. This is done by mapping each 1-D data point to a corresponding 2-D ordered pair. So for any non-linearly separable data in any dimension, we can just map the data to a higher dimension and then make it linearly separable. This is a very powerful and general transformation. A kernel is nothing but a measure of similarity between data points. The kernel function in a kernelized SVM tells you, that given two data points in the original feature space, what the similarity is between the points in the newly transformed feature space. There are various kernel functions available, but two are very popular :\n\n* Radial Basis Function Kernel (RBF): The similarity between two points in the transformed feature space is an exponentially decaying function of the distance between the vectors and the original input space as shown below. RBF is the default kernel used in SVM.\n\n* Polynomial Kernel: The Polynomial kernel takes an additional parameter, ‘degree’ that controls the model’s complexity and computational cost of the transformation","metadata":{}},{"cell_type":"code","source":"lin_svc = svm.LinearSVC().fit(x_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_classification(lin_svc, \"Linear SVC(LBasedImpl)\", x_train, x_test, y_train, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Decision Tree\nDecision Tree is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart-like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label. \n\n<img src= https://www.mastersindatascience.org/wp-content/uploads/sites/54/2022/05/tree-graphic.jpg width=500/>\n\nA tree can be “learned” by splitting the source set into subsets based on an attribute value test. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node all has the same value of the target variable, or when splitting no longer adds value to the predictions.Decision trees classify instances by sorting them down the tree from the root to some leaf node, which provides the classification of the instance. An instance is classified by starting at the root node of the tree, testing the attribute specified by this node, then moving down the tree branch corresponding to the value of the attribute as shown in the above figure. This process is then repeated for the subtree rooted at the new node. \nThe decision tree in above figure classifies a particular morning according to whether it is suitable for playing tennis and returning the classification associated with the particular leaf.(in this case Yes or No). ","metadata":{}},{"cell_type":"code","source":"dt = DecisionTreeClassifier(max_depth=3).fit(x_train, y_train)\ntdt = DecisionTreeClassifier().fit(x_train, y_train)\nevaluate_classification(tdt, \"DecisionTreeClassifier\", x_train, x_test, y_train, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def f_importances(coef, names, top=-1):\n    imp = coef\n    imp, names = zip(*sorted(list(zip(imp, names))))\n\n    # Show all features\n    if top == -1:\n        top = len(names)\n    \n    plt.figure(figsize=(10,10))\n    plt.barh(range(top), imp[::-1][0:top], align='center')\n    plt.yticks(range(top), names[::-1][0:top])\n    plt.title('feature importances for Decision Tree')\n    plt.show()\n\nfeatures_names = data_train.drop(['outcome', 'level'] , axis = 1)\nf_importances(abs(tdt.feature_importances_), features_names, top=18)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,12))\ntree.plot_tree(dt , filled=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random forest\n\n<img src=https://www.researchgate.net/publication/354354484/figure/fig4/AS:1080214163595269@1634554534720/Illustration-of-random-forest-trees.jpg width=600/>\n\nRandom forest is a supervised learning algorithm. The “forest” it builds is an ensemble of decision trees, usually trained with the “bagging” method. The general idea of the bagging method is that a combination of learning models increases the overall result.One big advantage of random forest is that it can be used for both classification and regression problems, which form the majority of current machine learning systems. and It also resists overfitting found in decision trees.","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier().fit(x_train, y_train)\nevaluate_classification(rf, \"RandomForestClassifier\", x_train, x_test, y_train, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f_importances(abs(rf.feature_importances_), features_names, top=18)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building an XGBOOST REgressor regressor in order to predict threat level","metadata":{}},{"cell_type":"code","source":"xg_r = xgb.XGBRegressor(objective ='reg:linear',n_estimators = 20).fit(x_train_reg, y_train_reg)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:49.50106Z","iopub.execute_input":"2022-09-16T14:44:49.501621Z","iopub.status.idle":"2022-09-16T14:44:55.062829Z","shell.execute_reply.started":"2022-09-16T14:44:49.50157Z","shell.execute_reply":"2022-09-16T14:44:55.061792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"name = \"XGBOOST\"\ntrain_error = metrics.mean_squared_error(y_train_reg, xg_r.predict(x_train_reg), squared=False)\ntest_error = metrics.mean_squared_error(y_test_reg, xg_r.predict(x_test_reg), squared=False)\nprint(\"Training Error \" + str(name) + \" {}  Test error \".format(train_error) + str(name) + \" {}\".format(test_error))","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:57.710767Z","iopub.execute_input":"2022-09-16T14:44:57.711199Z","iopub.status.idle":"2022-09-16T14:44:57.823601Z","shell.execute_reply.started":"2022-09-16T14:44:57.711161Z","shell.execute_reply":"2022-09-16T14:44:57.822543Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = xg_r.predict(x_test_reg)\ndf = pd.DataFrame({\"Y_test\": y_test_reg , \"Y_pred\" : y_pred})\nplt.figure(figsize=(16,8))\nplt.plot(df[:80])\nplt.legend(['Actual' , 'Predicted'])","metadata":{"execution":{"iopub.status.busy":"2022-09-16T14:44:58.171505Z","iopub.execute_input":"2022-09-16T14:44:58.173224Z","iopub.status.idle":"2022-09-16T14:44:58.523919Z","shell.execute_reply.started":"2022-09-16T14:44:58.173173Z","shell.execute_reply":"2022-09-16T14:44:58.522891Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Measuring effect of PCA","metadata":{}},{"cell_type":"code","source":"rrf = RandomForestClassifier().fit(x_train_reduced, y_train_reduced)\nevaluate_classification(rrf, \"PCA RandomForest\", x_train_reduced, x_test_reduced, y_train_reduced, y_test_reduced)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Neural networks\n\nNeural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\nArtificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n\n<img src= https://www.researchgate.net/publication/335903047/figure/fig1/AS:805611931443200@1569084260672/Proposed-artificial-neural-network-model-3-10-1-Neural-network-weight-vectors-are.png width=600/>\n\nNeural networks rely on training data to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the most well-known neural networks is Google’s search algorithm.","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=64, activation='relu', input_shape=(x_train.shape[1:]), \n                          kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), \n                          bias_regularizer=regularizers.L2(1e-4),\n                          activity_regularizer=regularizers.L2(1e-5)),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(units=128, activation='relu', \n                          kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), \n                          bias_regularizer=regularizers.L2(1e-4),\n                          activity_regularizer=regularizers.L2(1e-5)),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(units=512, activation='relu', \n                          kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), \n                          bias_regularizer=regularizers.L2(1e-4),\n                          activity_regularizer=regularizers.L2(1e-5)),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(units=128, activation='relu', \n                          kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), \n                          bias_regularizer=regularizers.L2(1e-4),\n                          activity_regularizer=regularizers.L2(1e-5)),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(units=1, activation='sigmoid'),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, verbose=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('SCCE Loss')\nplt.legend()\nplt.grid(True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keys = [key for key in kernal_evals.keys()]\nvalues = [value for value in kernal_evals.values()]\nfig, ax = plt.subplots(figsize=(20, 6))\nax.bar(np.arange(len(keys)) - 0.2, [value[0] for value in values], color='darkred', width=0.25, align='center')\nax.bar(np.arange(len(keys)) + 0.2, [value[1] for value in values], color='y', width=0.25, align='center')\nax.legend([\"Training Accuracy\", \"Test Accuracy\"])\nax.set_xticklabels(keys)\nax.set_xticks(np.arange(len(keys)))\nplt.ylabel(\"Accuracy\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keys = [key for key in kernal_evals.keys()]\nvalues = [value for value in kernal_evals.values()]\nfig, ax = plt.subplots(figsize=(20, 6))\nax.bar(np.arange(len(keys)) - 0.2, [value[2] for value in values], color='g', width=0.25, align='center')\nax.bar(np.arange(len(keys)) + 0.2, [value[3] for value in values], color='b', width=0.25, align='center')\nax.legend([\"Training Precesion\", \"Test Presision\"])\nax.set_xticklabels(keys)\nax.set_xticks(np.arange(len(keys)))\nplt.ylabel(\"Precesion\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keys = [key for key in kernal_evals.keys()]\nvalues = [value for value in kernal_evals.values()]\nfig, ax = plt.subplots(figsize=(20, 6))\nax.bar(np.arange(len(keys)) - 0.2, [value[2] for value in values], color='g', width=0.25, align='center')\nax.bar(np.arange(len(keys)) + 0.2, [value[3] for value in values], color='b', width=0.25, align='center')\nax.legend([\"Training Recall\", \"Test Recall\"])\nax.set_xticklabels(keys)\nax.set_xticks(np.arange(len(keys)))\nplt.ylabel(\"Recall\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}